{
    "contents" : "---\ntitle: \"Report\"\nauthor: \"Marco Bellan\"\ndate: \"24 settembre 2015\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(cache=TRUE)\noptions(warn=-1)\n```\n\n##Introduction\n\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). \n\n##Data\nThe training data for this project are available here:   \nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  \nThe test data are available here:   \nhttps://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  \nThe data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. \n\n##Loading data\n```{r}\nlibrary(caret)\n#Set a seed for coherent tests\nset.seed(12345)\ntraining = read.csv(\"Data/pml-training.csv\", header = TRUE)\nfinalTest = read.csv(\"Data/pml-testing.csv\", header = TRUE)\n```\n##Data Preprocessing\nWe split the preprocessing in two phases: a generic preprocessing and a trainingSet-specific preprocessing. \n\n####Generic preprocessing\nIn generic preprocessing we drop useless columns, we factorize the ```classe``` variable, and we set all the missing values to NA.\n```{r}\ntoDrop = c(\"user_name\",\"X\",\"raw_timestamp_part_1\",\"raw_timestamp_part_2\",\"cvtd_timestamp\",\"new_window\",\"num_window\")\ntraining = training [,!(names(training) %in% toDrop) ]\nfinalTest = finalTest [,!(names(finalTest) %in% toDrop)]\n\n#Convert training-set's \"Classe\" column to factor variable\ntraining[,\"classe\"] = as.factor(training[,\"classe\"])\n\n#Set null or empty values to NA\nis.na(training[,]) <- training[,names(training)] == \"\"\nis.na(training[,]) <- training[,names(training)] == \"NULL\"\n```\n\n####Data splitting\nWe proceed by splitting the data in a training and test set:\n```{r}\n#Split the training set with 60-40\ntrainIndexes <- createDataPartition(training$classe, p = .6,list = FALSE)\n#Remove \"classe\" since this should be treated as a test set, so without labels\ntest <- training[-trainIndexes,]\ntraining <- training[trainIndexes,]\n#Clean RAM\nrm(trainIndexes)\n```\n\n####TrainingSet-specific preprocessing\nHere we do all the preprocessing operations that depend on parameters that must be determined on the training set only, and then we apply them to the other sets aswell:  \nWe remove all the columns that have more than 50% of null values, since they could give problems with knnImputees  \nThen we create a preprocess object that applies knnImputees to fill in missing data, plus centering and scaling for algorithm efficiency.\nBoth PCA and NZV were considered in this case, but they weren't so usefull because of weak correlations and generally high variance between values.  \n```{r}\n#Find all the columns with less than 50% NA values\ntraining <- training[,(colSums(is.na(training[,names(training)])) / nrow(training) ) < 0.5]\n#Save this for later removing the same vars from the test set aswell\ntoKeep <- names(training)\ntest <- test [, toKeep]\ntoKeepTest <- toKeep[toKeep !=\"classe\"]\n#Keep the same variables, since we have to always use the same model\nfinalTest <- finalTest[,toKeepTest]\n\n\n#Create a preprocessing object using the training set\n#PCA and NZV were considered but they wouldn't be usefull in this case\n\n#Extract everything except the solution\ntr <- training[,!(colnames(training)==\"classe\")] \n#Preprocess\npreprocessing <- preProcess(tr,method=c(\"knnImpute\",\"center\",\"scale\"))\n#Save the solutions for the training set\nclasse <- training$classe\n#Clear memory\nrm(training)\n#Reassign the training set with the labels\ntraining <- predict(preprocessing,newdata=tr)\ntraining$classe <- classe\n#Remove the temporary variables\nrm(tr)\nrm(classe)\n\n#>Preprocess the test sets\nte <- test[,!(colnames(test)==\"classe\")]\nclasse <- test$classe\nrm(test)\ntest <- predict(preprocessing,newdata=te)\ntest$classe <- classe\n\nfinalTest <- predict(preprocessing,newdata=finalTest)\n\n#Remove the temporary variables\nrm(te)\nrm(classe)\n\n\n```\n####Validation set\nWe then process the test sets with the same procedure and then create a further split in the test data, to obtain a validation set.  \nThis hasn't been before for the ease of applying the same preprocessing operation to only one set.  \nWe keep 75% in test set cause we need it to train our combining predictor.\n```{r}\ntestIndexes <- createDataPartition(test$classe, p = .75,list = FALSE) \nvalidation <- test[-testIndexes,]\ntest <- test[testIndexes,]\nrm(testIndexes)\n```\n##Model fitting\nFirst of all, we declare a ```{r}trainControl``` object which will take care of repeating 5-fold CV 3 times in order to tune parameters.  \nThen we go on and train three models on the training set. We deceided to use multiple models for accademic purposes, since multi-model prediction was in the scope of the course.  \nThe three algorithms are:  \n-Penalized Multinomial Regression neural network, since NN represent the state of the art in deep learning techniques  \n-SVM (Support Vector Machine) which is one of the most recent and powerfull algorithms and LMC (Large Margin Classifier)  \n-Random Forest, for its versatility and ability to perform feature extraction, seen extensively in the course\n```{r, message=\"False\", results=\"hide\"}\n#Fit a NN algorithm\nNN <- train(training$classe ~ ., data=as.data.frame(training), method=\"multinom\",trainControl=trainControl, allowParallel=TRUE, verbose=FALSE)\n#Fit an SVM, one of the most advanced algorithms\nSVM <- train (training$classe ~ .,data=training,method=\"svmLinear\",trainControl=trainControl,allowParallel=TRUE, verbose=FALSE)\n#Fit a random forest, for its ability to detect usefull features and correlations\nRF <- train(training$classe ~ ., data=training, method=\"rf\", trainControl=trainControl,allowParallel=TRUE, verbose=FALSE)\n```\n##Model evalutation\nHere we use the models to predict the test set data, and then the ConfusionMatrix to evalutate their accuracy:\n```{r}\nSVMpredict <- predict(SVM, newdata=test)\nSVMaccuracy <- confusionMatrix(SVMpredict,test$classe)$overall['Accuracy']\nSVMaccuracy\n\nRFpredict <- predict(RF, newdata=test)\nRFaccuracy <- confusionMatrix(RFpredict,test$classe)$overall['Accuracy']\nRFaccuracy\n\nNNpredict <- predict(NN, newdata=test)\nNNaccuracy <- confusionMatrix(NNpredict,test$classe)$overall['Accuracy']\nNNaccuracy\n```\n##Combining models\nIn this section we use the models predictions on the test set, and the test set labels, to train a NaiveBayes classifier that can predict a result class based on the other three models predictions.  \nThis is usefull because the combined model can learn to take into account intrinsic errors in the fed-in data.  \n```{r}\n#Combine the predictors\ncombinedPredictors <- data.frame(SVMpredict,RFpredict,NNpredict,test$classe)\n#Train a predictor\ncombinedModel <- train(combinedPredictors$test.classe ~ ., data= combinedPredictors, method = \"nb\")\n```\n##Evalutating the combined model\nHere we use the three models to predict the labels for the validation set, then we frame those prediction and feed them to the combined model in order to evaluate its accuracy in an out-of-sample set:\n```{r}\n#Predict for validation\nSVMpredict <- predict(SVM, newdata= validation)\nRFpredict <- predict(RF, newdata= validation)\nNNpredict <- predict(NN, newdata= validation)\n\n#Combine the data\ncombinedPredictors <- data.frame(SVMpredict,RFpredict,NNpredict)\ncombinedPredictors$test.classe <- validation$classe\n\n#Make it predict for evalutation\ncombinedPrediction <- predict(combinedModel,newdata=combinedPredictors)\ncombinedAccuracy <- confusionMatrix(combinedPrediction,validation$classe)$overall['Accuracy']\ncombinedAccuracy\n```\n\n##Predicting the final labels\nHere we use the three models plus the combined model to predict the result of the ```{r}finalTest``` set, which is the one provided for the assignment. \n```{r}\nrm(SVMpredict)\nrm(RFpredict)\nrm(NNpredict)\n\n#Measure time to see how long it takes\nstartTime<- proc.time()\n\nSVMpredict <- predict(SVM, newdata=finalTest)\nRFpredict <- predict(RF, newdata= finalTest)\nNNpredict <- predict(NN, newdata= finalTest)\n\nfinalCombinedData<- data.frame(SVMpredict,RFpredict,NNpredict)\n\noutCome <- predict(combinedModel, finalCombinedData)\n\nproc.time() - startTime\n\noutCome\n```\n\n##Conclusion\nThe combined model accuracy is slightly lower than the RandomForest one.  \nAs i said, i went for a multimodel classifier just for didactical purposes.  \nRandomForest proves again to be a great algorithm at the expense of very long training times, with more than 99% accuracy out-of-sample.  \nIn the end, all the results predicted by the algorithm are correct, and the final prediction is really fast aswell.  \nSome future work may include some optimization, parameters tuning and maybe a deeper experimentation in various types of models.  \nIt is also reccomended to port this algorithm in another programming language for practical purposes.\n\n###Clarification on style\nYou may notice I use the \"we\" pronoun when i think of the choiches made for this algorithm, this is something i'm used to.  \nAll the work is done solely by me, Marco Bellan, using notions acquired from the Practical Machine Learning Course, along with the R and Caret wiki and documentation.\n",
    "created" : 1443112039377.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3169513859",
    "id" : "C5AB16F9",
    "lastKnownWriteTime" : 1443110958,
    "path" : "C:/Users/Marco/Desktop/R/CourseProject/Report.Rmd",
    "project_path" : "Report.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}